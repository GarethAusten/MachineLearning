{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%load_ext autotime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AutoEncoders 101 \n",
    "\n",
    "AutoEncoders typically have a hidden layer $h$ that represents the input vector $\\textbf{x}$. Autoencoders consist of two parts, an encoder that models $h = f(x)$ and a decoder that produces a reconstruction $r = g(h)$. AutoEncoders are trained in the hope that $h$ will take on a useful representation of the training data and are therefore usually restricted in some way to prevent the decoder from learning to perfectly reconstruct the input. \n",
    "\n",
    "AutoEncoders are trained to minimize a loss objective such as\n",
    "\n",
    "$$\n",
    "L(x, g(f(x)))\n",
    "$$\n",
    "\n",
    "Where L is a loss function such as cross entropy loss. \n",
    "\n",
    "## Denoising AutoEncoders (DAEs)\n",
    "\n",
    "Denoising AutoEncoders are very similar to AutoEncoders with a slight variation to the loss function: \n",
    "\n",
    "$$\n",
    "L(x, g(f(\\tilde{x})))\n",
    "$$\n",
    "\n",
    "where $\\tilde{x}$ is a copy of $\\textbf{x}$ that has undergone some sort of perturbation to corrupt the copy. This introduces noise to the training data and helps to prevent the autoencoder from learning the identity function.  \n",
    "\n",
    "## Variational AutoEncoders\n",
    "\n",
    "Variational AutoEncoders (VAEs) were introduced by Diederik Kingma and Max Welling in 2013. \n",
    "\n",
    "Key innovation is that they can be trained to maximize the variational lower bound $L(q)$ w.r.t x:\n",
    "\n",
    "$$\n",
    "\\mathrm{L}(q) = \\mathbb{E}_{z \\sim q(z | x)}log_{p_model}(z|x) + H(q(z|x)) \\\\\n",
    "= \\mathbb{E}_{z \\sim q(z | x)}logp_{model}(z|x) - D_{KL}(q(z|x)||p_{model}(z)) \n",
    "$$\n",
    "\n",
    "The first term is the reconstruction loss found in other autoencoders while the second term tries to make the approximate posterior distribution $q(z | x)$ and the model prior $p_{model}(z)$ approach each other. \n",
    "\n",
    "By choosing $\\mathbf{q}$ to be gaussian and noise added to the predicted mean. This encourages the VAE to place high probability mass on many Z values rather than focusing on the most likely point. \n",
    "\n",
    "Benefits: \n",
    "\n",
    "* Can represent much more complex relationships than traditional dimensionality reduction e.g., PCA\n",
    "* No need for MCMC\n",
    "* q is user defined \n",
    "\n",
    "### The reparameterization trick \n",
    "\n",
    "The reparameterization trick allows the second term in the loss function to be computed analytically by assuming the posterior has a Gaussian distribution with added noise. Reparameterizing $z$ as:\n",
    "\n",
    "$$\n",
    "z = \\mathbf{\\epsilon}\\mathbf{\\sigma_x} + \\mu_x\n",
    "$$\n",
    "\n",
    "\n",
    "<img src=\"./www/reparam_trick.png\" alt=\"Reparameterization Trick\" style=\"width: 400px;\"/>\n",
    "\n",
    "\n",
    "## Adversarial AutoEncoders\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 9.04 ms\n"
     ]
    }
   ],
   "source": [
    "[1] * 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resources/References\n",
    "\n",
    "[1 - AutoEncoding Variational Bayes](https://arxiv.org/abs/1312.6114)\n",
    "\n",
    "[2 - Deep Learning Chapter 14, Goodfellow](https://www.deeplearningbook.org/contents/autoencoders.html)\n",
    "\n",
    "[3 - Adversarial AutoEncoders](https://arxiv.org/abs/1511.05644)\n",
    "\n",
    "[4 - An Introduction to Variational AutoEncoders](https://arxiv.org/pdf/1906.02691.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Efficient Natural Language Response Suggestion for Smart Reply\n",
    "\n",
    "Link: [Paper](https://arxiv.org/pdf/1705.00652.pdf)\n",
    "\n",
    "**Summary:**\n",
    "\n",
    "This paper describes improvements to the initial smart reply model proposed by Kanaan et al. Improvements are aimed specifically at reducing the computationally complexity of training the smart reply system and reducing latency at inference time. This is achieved by using a feed-forward neural network to learn embeddings and extract a high dot-product between a new message and the set of possible responses ($R$). The authors keep two of the key components from the initial smart reply system, the triggering model and the diversity module based on the EXPANDER algorithm, and instead focus on improving the response selection. \n",
    "\n",
    "**Innovations:**\n",
    "\n",
    "* Using a feedforward network to score responses in place of a generative model to reduce computational cost.\n",
    "    + N-gram embeddings are used to approximate sequences and captures basic semantic and word ordering information\n",
    "* Multiple Negatives \n",
    "    + Given a batch size of K possible responses each sample in the batch is treated as having K-1 negatives. \n",
    "* Hierarchical Quantization\n",
    "    + Gives further efficiency improvements when searching for the best responses in the candidate space.\n",
    "    \n",
    "## Dot Product Model \n",
    "\n",
    "THe authors describe a dot product scoring model where $S(x,y)$ is factorized as a dot product between vector $\\textbf{h_x}$ that depends only on x and a vector $\\textbf{h_y}$ that depends only on 1. This is represented as figure 3 (b) in the paper and is shown below. \n",
    "\n",
    "![Dot Product Scoring Model](./www/dot_prod_score.png)\n",
    "\n",
    "This can be implemented in PyTorch as shown below. The stacks are identical so the model can simply be assigned twice. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dotProdModel(nn.Module):\n",
    "    \"\"\"Torch Dot Model.\"\"\"\n",
    "    \n",
    "    def __init__(self, hidden_size1, hidden_size2, hidden_size3, \n",
    "                 vocab_size, dropout, pretrained=False, weights=None, \n",
    "                 emb_dim=None):\n",
    "        \"\"\"Initialization.\"\"\"\n",
    "        super(dotModel, self).__init__()\n",
    "        \n",
    "        if pretrained:\n",
    "            self.embedding = nn.Embedding(weights.size(0), weights.size(1))\n",
    "            self.embedding.weight.data.copy_(weights)\n",
    "            emb_dim = weights.size(1)\n",
    "        else:\n",
    "            self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "            \n",
    "        self.linear1 = nn.Linear(emb_dim, hidden_size1)\n",
    "        self.linear2 = nn.Linear(hidden_size1, hidden_size2)\n",
    "        self.linear3 = nn.Linear(hidden_size2, hidden_size3)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.activation = nn.Tanh()\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass.\"\"\"\n",
    "        h = torch.sum(self.embedding(x), dim=0)\n",
    "        h = self.dropout(self.activation(self.linear1(h)))\n",
    "        h = self.dropout(self.activation(self.linear2(h)))\n",
    "        h = self.dropout(self.activation(self.linear3(h)))\n",
    "        \n",
    "        return h\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple Negatives and the Loss Function\n",
    "\n",
    "For efficiency, a set of K possible responses is used to approximate $P(y|x)$, one correct response and k-1 random negatives. For simplicity, they use the responses of other examples in a training batch of stochastic gradient descent as negative responses. For a batch size of $K$, there will be $K$ input emails $\\textbf{x} = (x_1, ..., x_K)$ and their corresponding responses $\\textbf{y} = (y_1, ..., y_K)$. Every reply $y_j$ is effectively treated as a negative candidate for $x_i$ if $i \\neq j$. The K-1 negative examples for each $x$ are different at each pass through the data due to shuffling in stochastic gradient decent. The goal of training is to minimize the approximated mean negative log probability of the data. For a single batch this is:\n",
    "\n",
    "$$\n",
    "\\jmath (\\textbf{x}, \\textbf{y}, \\theta) = -\\frac{1}{K}\\sum_{i=1}^{K} [S(x_i, y_i) - log \\sum_{j=1}^{K} e^{S(x_i, y_j)}]\n",
    "$$\n",
    "\n",
    "This is implemented in PyTorch as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class approxMeanNegativeLoss(nn.Module):\n",
    "    \"\"\"Loss function.\"\"\"\n",
    "    def __init__(self):\n",
    "        super(approxMeanNegativeLoss, self).__init__()\n",
    "        \n",
    "    def forward(self, src_pos, trg_pos, batch_size):\n",
    "            try:\n",
    "                assert batch_size == src_pos.size()[0]\n",
    "            except AssertionError:\n",
    "                batch_size = src_pos.size()[0]\n",
    "            S_xi_yi = torch.mm(src_pos, trg_pos.t()).diag()\n",
    "            log_sum_exp_S = torch.log(torch.sum(torch.exp(torch.mm(src_pos, trg_pos.t())), dim=1))\n",
    "            return -(((S_xi_yi - log_sum_exp_S).sum()) / batch_size) + 1e-9"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

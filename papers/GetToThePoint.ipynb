{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get to the Point Summarization with Pointer Generators\n",
    "\n",
    "2017 [paper](https://arxiv.org/pdf/1704.04368.pdf) ([code](https://arxiv.org/pdf/1704.04368.pdf))which augments the attention sequence to sequence model for text summarization in two key ways:\n",
    "\n",
    "1. Use of a hybrid pointer-generator network that can copy words from the source text via pointing, which aids accurate reproduction of information, while retaining the ability to produce novel words through the generator\n",
    "2. Use of coverage which keeps track of what has been summarized to discourage repetition\n",
    "\n",
    "The complete network can be seen below:\n",
    "\n",
    "![Pointer Generator Network](./img/pointer_generator_network.png)\n",
    "\n",
    "## Breaking down the model\n",
    "\n",
    "The Pointer Generator model builds on top of a simple sequence to sequence model with attention. The tokens of the article, $w_i$ are fed one-by-one into the encoder (a single layer bi-directional LSTM), producing a sequence of _encoder hidden states $h_i$._ \n",
    "\n",
    "### The Encoder Module\n",
    "\n",
    "Below is a PyTorch implementation of this encoder module\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    \"\"\"Encoder RNN Module.\"\"\"\n",
    "    def __init__(self, metadata, hidden_dim, num_layers=1, rnn_cell=nn.LSTM, bidirectional=True):\n",
    "        \"\"\"\n",
    "\n",
    "        :param metadata: Dictionary containing metadata about the inputs \n",
    "        :param hidden_dim: Number of hiddens dimensions in the RNN\n",
    "        :param bidirectional: bool - RNN \n",
    "        \"\"\"\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.metadata = metadata\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.embedding = nn.Embedding(self.metadata['fields']['src'].vocab.vectors.size(0),\n",
    "                                      self.metadata['fields']['src'].vocab.vectors.size(1))\n",
    "        self.embedding.weight.data.copy_(self.metadata['fields']['src'].vocab.vectors)\n",
    "        emb_dim = self.embedding.weight.size(1)\n",
    "        self.directional_multiplier = int(bidirectional) + 1\n",
    "        self.rnn = rnn_cell(emb_dim, hidden_dim, num_layers=self.num_layers, batch_first=True, bidirectional=bidirectional)\n",
    "        self.W_h = nn.Linear(hidden_dim * self.directional_multiplier,\n",
    "                             hidden_dim * self.directional_multiplier,\n",
    "                             bias=False)\n",
    "\n",
    "    def forward(self, inputs, seq_lens):\n",
    "        \"\"\"\n",
    "        Make a forward pass through the encoder.\n",
    "        \n",
    "\n",
    "        :param inputs: Input text. Torch tensor of vocab IDs\n",
    "        :param seq_lens: Lenght of input sequences \n",
    "        :return:\n",
    "            encoder_outputs: The encoder final hidden for all time steps\n",
    "            encoder_feature: \n",
    "        \"\"\"\n",
    "        embeddings = self.embedding(inputs)\n",
    "        packed = pack_padded_sequence(embeddings, seq_lens, batch_first=True)\n",
    "        output, hidden = self.rnn(packed)\n",
    "        encoder_outputs, _ = pad_packed_sequence(output, batch_first=True)\n",
    "        encoder_outputs = encoder_outputs.contiguous()\n",
    "\n",
    "        encoder_feature = encoder_outputs.view(-1, self.directional_multiplier * self.hidden_dim)\n",
    "        encoder_feature = self.W_h(encoder_feature)\n",
    "\n",
    "        return encoder_outputs, encoder_feature, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bahdanau Attention Mechanism\n",
    "\n",
    "Attention is implemented as in [Bahdanau et al](https://arxiv.org/pdf/1409.0473.pdf)\n",
    "\n",
    "$$\n",
    "e_i^t = v^T tanh(W_hh_i + W_ss_i + b_{atttn}) \\\\\n",
    "\\alpha_t = softmax(e^t)\n",
    "$$\n",
    "\n",
    "Where,\n",
    "\n",
    "* $v, W_h, W_s, b_{attn}$ are all learneable parameters\n",
    "* $h_i$ is the sequence of encoder hidden states\n",
    "* $s_t$ is the decoder hidden state\n",
    "* $t$ corresponds to each step in the decoder\n",
    "\n",
    "The attention distribution $\\alpha^t$ is calculated at each decoder step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(nn.Module):\n",
    "    \"\"\"Bahdanau Attention Mechanism.\"\"\"\n",
    "    def __init__(self, hidden_dim, is_coverage=False):\n",
    "        \"\"\"\n",
    "\n",
    "        :param hidden_dim:\n",
    "        \"\"\"\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.is_coverage = is_coverage\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        if self.is_coverage:\n",
    "            self.W_c = nn.Linear(1, hidden_dim * 2, bias=False)\n",
    "        \n",
    "        self.decode_proj = nn.Linear(self.hidden_dim * 2, self.hidden_dim * 2)\n",
    "        self.v = nn.Linear(hidden_dim * 2, 1, bias=False)\n",
    "\n",
    "    def forward(self, s_t_hat, encoder_outputs, encoder_feature, enc_padding_mask, coverage):\n",
    "        \"\"\"\n",
    "        Calculation of the Attention probability distribution as outlined by Bahdanau et al.\n",
    "\n",
    "        :param s_t_hat: Decoder State\n",
    "        :param encoder_outputs: Encoder hidden states\n",
    "        :param encoder_feature: Encoder weight matrix\n",
    "        :param enc_padding_mask: Encoder Mask\n",
    "        :param coverage:\n",
    "        :return:\n",
    "            c_t: context vector (weighted sum of the encoder hidden states\n",
    "            attn_dist: the attention distribution over the encoder vocabulary\n",
    "            coverage: Coverage vector which is the sume of the attention distribution over all previous time steps\n",
    "        \"\"\"\n",
    "        b, t_k, n = list(encoder_outputs.size())\n",
    "\n",
    "        dec_fea_expanded = self.decode_proj(s_t_hat).unsqueeze(1).expand(b, t_k, n).contiguous()\n",
    "        dec_fea_expanded = dec_fea_expanded.view(-1, n)  # B * t_k x 2*hidden_dim\n",
    "\n",
    "        att_features = encoder_feature + dec_fea_expanded # B * t_k x 2*hidden_dim\n",
    "        if self.is_coverage:\n",
    "            coverage_input = coverage.view(-1, 1)  # B * t_k x 1\n",
    "            coverage_feature = self.W_c(coverage_input)  # B * t_k x 2*hidden_dim\n",
    "            att_features = att_features + coverage_feature\n",
    "            \n",
    "        scores = self.v(torch.tanh(att_features)).view(-1, t_k)\n",
    "\n",
    "        attn_dist = F.softmax(scores, dim=1)*enc_padding_mask.to(dtype=torch.float32) # B x t_k\n",
    "        normalization_factor = attn_dist.sum(1, keepdim=True)\n",
    "        attn_dist = attn_dist / normalization_factor\n",
    "\n",
    "        attn_dist = attn_dist.unsqueeze(1)  # B x 1 x t_k\n",
    "        c_t = torch.bmm(attn_dist, encoder_outputs)  # B x 1 x n\n",
    "        c_t = c_t.view(-1, self.hidden_dim * 2)  # B x 2*hidden_dim\n",
    "\n",
    "        attn_dist = attn_dist.view(-1, t_k)  # B x t_k\n",
    "\n",
    "        if self.is_coverage:\n",
    "            coverage = coverage.view(-1, t_k)\n",
    "            coverage = coverage + attn_dist\n",
    "\n",
    "        return c_t, attn_dist, coverage\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The attention distribution is used to produce a weighted sum of the encoder hidden states, known as the context vector $h_t^*$:\n",
    "\n",
    "$$\n",
    "h_t^* = \\sum_i \\alpha_i^th_i\n",
    "$$\n",
    "\n",
    "This context vector which is seen as a fixed size representation of what has been read from the source for this step, is concatenated with the decoder state $s_t$ and fed through two linear layers to produce the vocabulary distribution $P_{vocab}$:\n",
    "\n",
    "$$\n",
    "P_{vocab} = softmax(V^{\\prime}(V[s_t, h_t^*] + b) + b^{\\prime})\n",
    "$$\n",
    "\n",
    "where,\n",
    "\n",
    "* $V^{\\prime}, V, b^{\\prime}, b$ are learneable parameters. \n",
    "\n",
    "$P_{vocab}$ is the probability distribution over all words in the vocab. During training the loss for each timestep t is the negative log likelihood of the target word $w_t^*$ for that timestep:\n",
    "\n",
    "$$\n",
    "loss_t = -log P(w_t^*)\n",
    "$$\n",
    "\n",
    "and the overall loss for the whole sequence is:\n",
    "\n",
    "$$\n",
    "loss = \\frac{1}{T}\\sum_{t=0}^T loss_t\n",
    "$$\n",
    "\n",
    "### Pointer Generator Components\n",
    "\n",
    "Generation probablity $p_{gen} \\in [0,1]$ for timestemp $t$ is calculated from the context vector $h_t^*$, the decoder state $s_t$ and the decoder input $x_t$:\n",
    "\n",
    "$$\n",
    "p_{gen} = \\sigma(w_{h^*}^th_t^* + w_s^Ts_t + w_x^Tx_t + b_{ptr})\n",
    "$$\n",
    "\n",
    "where vectors $w_{h^*}, w_s, w_x, b_{ptr}$ are learneable parameters and $\\sigma$ is the sigmoid function. $p_{gen}$ is used as a soft switch to decide between generating a word from $P_{vocab}$ or copying a word from the input sequence by sampling from the attention distribution $\\alpha^t$. For each document let the _vocabulary_ denote the union of the vocabulary and all the words appearing in the source document. \n",
    "\n",
    "The following distribution can be obtained over the extended vocabulary. \n",
    "\n",
    "$$\n",
    "P(w) = p_{gen}P_{vocab}(w) + (1 - P_{gen})\\sum_{i:w_i=w}\\alpha_i^t\n",
    "$$\n",
    "\n",
    "If $w$ is an out-of-vocabulary (OOV) word than $P_{vocab}$ is always zero. Similarly, if $w$ is not in the source document then $\\sum_{i:w_i=w}\\alpha_i^t$ is zero. The ability to produce OOV words is one of the primary advantages of Pointer-Generator networks. The loss function is as described above but with respect to the newly defined $P(w)$\n",
    "\n",
    "### Coverage Mechanism\n",
    "\n",
    "Repetition is common problem in generative seq2seq models. To solve this problem a coverage model is adapted. To solve this problem a coverage vector, $c^t$ is mainitained which is the sum of the attention distributions over all previous decoder steps:\n",
    "\n",
    "$$\n",
    "c^t = \\sum_{t^{\\prime}=0}^{t-1}\\alpha^{t^{\\prime}}\n",
    "$$\n",
    "\n",
    "Intuitively, $c^t$ is a unnormalized distribution over the source document words which represents the degree of coverage which those words have received so far. Note that $c^0$ is a zero vector as non of the words have received coverage so far. The coverage vector is an extra input to the attention mechaism changing the attention equation to:\n",
    "\n",
    "$$\n",
    "e_i^t = v^Ttanh(W_hh_i + W_ss_t + w_cc_i^t + b_{attn}) \n",
    "$$\n",
    "\n",
    "where $w_c$ is a learnable parameter vector of the same length as $v$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    \"\"\"Decoder Module for Pointer-Generator.\"\"\"\n",
    "    def __init__(self, metadata, hidden_dim, attention, vocab_size, rnn_cell=nn.LSTM, pointer_gen=True):\n",
    "        \"\"\"\n",
    "\n",
    "        :param metadata:\n",
    "        :param emb_dim:\n",
    "        :param hidden_dim:\n",
    "        :param rnn_cell:\n",
    "        \"\"\"\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.attention_network = attention\n",
    "        self.metadata = metadata\n",
    "        self.pointer_gen = pointer_gen\n",
    "        self.hidden_dim = hidden_dim\n",
    "        #self.embedding = self.metadata['EmbeddingModuleDict']['trg']\n",
    "        self.embedding = nn.Embedding(self.metadata['fields']['src'].vocab.vectors.size(0),\n",
    "                                      self.metadata['fields']['src'].vocab.vectors.size(1))\n",
    "        self.embedding.weight.data.copy_(self.metadata['fields']['src'].vocab.vectors)\n",
    "        emb_dim = self.embedding.weight.size(1)\n",
    "\n",
    "        self.x_context = nn.Linear(hidden_dim * 2 + emb_dim, emb_dim)\n",
    "\n",
    "        self.rnn = rnn_cell(emb_dim, hidden_dim, num_layers=1, batch_first=True, bidirectional=False)\n",
    "\n",
    "        if self.pointer_gen:\n",
    "            self.p_gen_linear = nn.Linear(self.hidden_dim * 4 + emb_dim, 1)\n",
    "\n",
    "        #p_vocab\n",
    "        self.out1 = nn.Linear(self.hidden_dim * 3, self.hidden_dim)\n",
    "        self.out2 = nn.Linear(self.hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, y_t_1, s_t_1, encoder_outputs, encoder_feature, enc_padding_mask,\n",
    "                c_t_1, extra_zeros, enc_batch_extend_vocab, coverage, step):\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        :param y_t_1: Single token input tensor [batch_size x 1]\n",
    "        :param s_t_1:\n",
    "        :param encoder_outputs:\n",
    "        :param encoder_feature:\n",
    "        :param enc_padding_mask:\n",
    "        :param c_t_1:\n",
    "        :param extra_zeros:\n",
    "        :param enc_batch_extend_vocab:\n",
    "        :param coverage:\n",
    "        :param step:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        if not self.training and step == 0:\n",
    "            h_decoder, c_decoder = s_t_1\n",
    "            s_t_hat = torch.cat((h_decoder.view(-1, self.hidden_dim),\n",
    "                                 c_decoder.view(-1, self.hidden_dim)), 1)  # B x 2*hidden_dim\n",
    "            c_t, _, coverage_next = self.attention_network(s_t_hat, encoder_outputs, encoder_feature,\n",
    "                                                           enc_padding_mask, coverage)\n",
    "            coverage = coverage_next\n",
    "\n",
    "        y_t_1_embd = self.embedding(y_t_1)\n",
    "        x = self.x_context(torch.cat((c_t_1, y_t_1_embd), 1))\n",
    "        lstm_out, s_t = self.rnn(x.unsqueeze(1), s_t_1)\n",
    "\n",
    "        h_decoder, c_decoder = s_t\n",
    "        s_t_hat = torch.cat((h_decoder.view(-1, self.hidden_dim),\n",
    "                             c_decoder.view(-1, self.hidden_dim)), 1)  # B x 2*hidden_dim\n",
    "        c_t, attn_dist, coverage_next = self.attention_network(s_t_hat, encoder_outputs, encoder_feature,\n",
    "                                                               enc_padding_mask, coverage)\n",
    "\n",
    "        if self.training or step > 0:\n",
    "            coverage = coverage_next\n",
    "\n",
    "        p_gen = None\n",
    "        if self.pointer_gen:\n",
    "            p_gen_input = torch.cat((c_t, s_t_hat, x), 1)  # B x (2*2*hidden_dim + emb_dim)\n",
    "            p_gen = self.p_gen_linear(p_gen_input)\n",
    "            p_gen = torch.sigmoid(p_gen)\n",
    "\n",
    "        output = torch.cat((lstm_out.view(-1, self.hidden_dim), c_t), 1)  # B x hidden_dim * 3\n",
    "        output = self.out1(output)  # B x hidden_dim\n",
    "\n",
    "        # output = F.relu(output)\n",
    "\n",
    "        output = self.out2(output)  # B x vocab_size\n",
    "        vocab_dist = F.softmax(output, dim=1)\n",
    "\n",
    "        if self.pointer_gen:\n",
    "            vocab_dist_ = p_gen * vocab_dist\n",
    "            attn_dist_ = (1 - p_gen) * attn_dist\n",
    "\n",
    "            if extra_zeros is not None:\n",
    "                vocab_dist_ = torch.cat([vocab_dist_, extra_zeros], 1)\n",
    "\n",
    "            final_dist = vocab_dist_.scatter_add(1, enc_batch_extend_vocab, attn_dist_)\n",
    "        else:\n",
    "            final_dist = vocab_dist\n",
    "\n",
    "        return final_dist, s_t, c_t, attn_dist, p_gen, coverage\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
